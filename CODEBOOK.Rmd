---
title: "Coursera *Getting and Cleaning Data* Course Project CODEBOOK"
author:  Nick T.
output: html_notebook
---

```{r, load_libraries, echo = FALSE, warning = FALSE, message = FALSE}
  ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
      install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
  }
  # Install and Load Packages
  packages <- c("purrr",
                "rio", 
                "tidyverse",
                "stringr")
  ipak(packages)
```

```{r, load_data, echo = FALSE, warning = FALSE, message = FALSE}
features <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/features.txt") # import features.txt data into working environment
x_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/x_test.txt") # import features.txt data into working environment

```

# Preface 

The Coursera course *Getting and Cleaning Data* CODEBOOK is intended to provide the user with enough information to replicate the data analysis in the project.  Users of the CODEBOOK should be able to understand how the data was tidyed to its current state.  

The CODEBOOK is a companion to the `run_analysis.R` and `README.md` files.  The github repository is located at https://github.com/prosumer1001/GettingAndCleaningData_Project.  Direct all questions to the author at prosumer1001@gmail.com.

# Instructions

### Review Criteria

1. The submitted data set is tidy.
2. The Github repo contains the required scripts.
3. GitHub contains a code book that modifies and updates the available codebooks with the data to indicate all the variables and summaries calculated, along with units, and any other relevant information.
4. The README that explains the analysis files is clear and understandable.
5. The work submitted for this project is the work of the student who submitted it.

### Course Project Description 

The course project description is copied verbatim from the site.[^1]

The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit: 1) a tidy data set as described below, 2) a link to a Github repository with your script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called `CodeBook.md`. You should also include a `README.md` in the repo with your scripts. This `README.md` explains how all of the scripts work and how they are connected.

One of the most exciting areas in all of data science right now is wearable computing - see for example <a href="http://www.insideactivitytracking.com/data-science-activity-tracking-and-the-battle-for-the-worlds-top-sports-brand/" target="_blank" rel="noopener nofollow">this article </a>. Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:

http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones

Here are the data for the project:

https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip

You should create one R script called run_analysis.R that does the following.

Merges the training and the test sets to create one data set.
Extracts only the measurements on the mean and standard deviation for each measurement.
Uses descriptive activity names to name the activities in the data set
Appropriately labels the data set with descriptive variable names.
From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

# CODEBOOK

The data codebook can be obtained from the author's .git repository, `GettingAndCleaningData_Project` titled `HARreadme.txt`.

# Step 1. Process different datasets.  

First, I take the `raw` datasets and transofrm them into individual `processed` datasets.  The first is the `features.txt` data.  The comments in the code follow the process for creating the first dataframe named `complete.RData`, which is saved in the `./Project/Data/Processed` directory.  this step creates the naming convention for the features included in the completed dataframe.  The naming conventions form the original features_info.txt file provies the most succinct and descriptive naming.  Quoting from the features_info.txt file:

> The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix 't' to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. 

> Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 

> Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the 'f' to indicate frequency domain signals). 

> These signals were used to estimate variables of the feature vector for each pattern:  
'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.

The difference between the original file and the naming conventions used in this dataframe include the removal of all special characters (i.e. **-** and **()**).  Additionally, all characters were changed to lower case.  Thus, a prior exmaple might be:  `fBodyAccJerkMag` and the new variable name is `fbodyaccjerkmag`.  While the lack of upper case letters may make deciphering the variable more difficult at first, it will make errors in the code of future uesers less likely to occur.  The practical benefits is that a major transformation has not taken place, so using the original code-book ensures replicability with less errors, which should increase the likelyhood of replication, all else equal.

The steps used to transform this data are as follows:

1. Load the features and x-test data into the local environment.
2. Remove all punctuation and change characters to lower case.
3. Rename vairalbes and use `select()` to choose only the `featureNames` variable for later use.
4. Make the vector `featureNames` from the prior step into the column names for the `x_test` dataframe.
5. Create a new processed dataframe called `completeDF` that will be joined later by the `partialDF` dataframe.
6. Add an ID row to the dataframe for later join with `partialDF`.
7. Save the `comleteDF` as an RDataframe for ease of use.



```{r, features, message = FALSE, warning=FALSE}
features <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/features.txt") # import features.txt data into working environment
x_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/x_test.txt") # import features.txt data into working environment
features$V2 <- str_replace_all(features$V2, "[[:punct:]]", "") # remove all punctuation and ensure there are no spaces
features$V2 <- tolower(features$V2)  # make all alpha characters lowercase
features <- features %>%  # change names of variables and select only the strings
        rename(objectid = "V1", featureNames = "V2") %>%
        select(featureNames)
names(x_test) <- features$featureNames # make the vector featureNames the column names for x_test
completeDF <- x_test # create new processed dataframe called `completeDF` that will ultimately have all the test data included once processed
completeDF <- tibble::rowid_to_column(completeDF, "ID")
export(completeDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/completeDF.RData") # save processed data to appropriate directory
```


# Step 2. Process the y_test.txt dataset, the activity_labels.txt dataset, and the subject_test.txt data sets.  

These provide a different challenge than the x_test data wrangling in Step 1, which requried transposing a vector and turning it into column names.  In this sequence of data wrangling the subjects, identified by an integer (1-30), needed to be married to the different factor values for walking, walking upstairs, walking downstairs, sitting, standing, and laying down.  The six categories are factors, since one can realistically understanding that lying down is less arduous than ascending stairs.  

The steps needed to complete Step 2 are as follows:

1. Load the subject, y-test data, and activity labels data in the local environment.
2. Create `ID` variables in the y-test and subject dataframes so they can be joined by the same.
3. Conduct a `dplyr` `left_join()` by the `ID` variables created in the prior step; for the case of this dataframe I decided to call the dataframe `partialDF`, which will ultimately join with the `completeDF` from Step 1 above.
4. Rename the variables created by the data import and left join.  The new variable names are `factor` and `subject` in the partialDF dataframe.  The activity lables dataframe will also receive a variable named `factor` that is synonomous with the `partialDF` factor variable.  The two dataframes will be left joined in the next step by the `factor` variable.  Finally, the activity label's second variable is renamed to `activitycategory`.  The names were chosen to be as simple as possible and follow the naming convention for the x-test variable from Step 1.
5. Conduct a `left_join()` between `activity_labels` dataframe and the `partialDF` dataframe by the `factor` variable.  A check of the dataframe with `sample_n()` form `dplyr` verifies the appropriate factors are matched with the activity categories.
6. Finally, exporting the `partialDF` to the `./Project/Data/Processed` directory ensures data replication and makes future joins easier to work with.

```{r, factor_observations, message = FALSE, warning=FALSE}
subject_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/subject_test.txt") ## load the subject_text.txt data file
y_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/y_test.txt") ## load the y_test.txt data file
activity_labels <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/activity_labels.txt") ## load the activity_labels.txt data file
# unique(activity_labels)
y_test <- tibble::rowid_to_column(y_test, "ID") ## create a unique variable named `ID` in the y_test for left_join(by = "ID")
subject_test <- tibble::rowid_to_column(subject_test, "ID") ## create a unique variable named `ID` in the subject_test for left_join(by = "ID")
partialDF <- left_join(y_test, subject_test, by = "ID") ## left_join y_test and subject_test by = "ID" to make the partialDF dataframe
partialDF <- partialDF %>% rename(factor = V1.x) ## rename the V1.x vairable to `factor` for left_join with activity_labels later
partialDF <- partialDF %>% rename(subject = V1.y)  ## rename V1.y as the subject number for clarity
activity_labels <- activity_labels %>% rename(factor = V1) ## rename the V1 variable from activity_lables to factor for left_join with partialDF
activity_labels <- activity_labels %>% rename(activitycategory = V2) ## rename V2 from activity_labels to acticity_category for clear variable description
# str(list(partialDF, activity_labels)) 
partialDF <- left_join(partialDF, activity_labels, by = "factor"); str(partialDF)  ## conduct left_join(by = "factor") for the partialDF and activity_lables described above; use str() to review the outcome of the join
sample_n(partialDF, 25) ## take a sample of the partialDF to make sure that the DF is complete and accurate
export(partialDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/partialDF.RData") # save processed data to appropriate directory

```




[^1]: from https://www.coursera.org/learn/data-cleaning/peer/FIZtT/getting-and-cleaning-data-course-project

