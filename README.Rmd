---
title: "Coursera *Getting and Cleaning Data* Course Project CODEBOOK"
author:  Nick T.
output: html_notebook
---

```{r, load_libraries, echo = FALSE, warning = FALSE, message = FALSE}
  ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
      install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
  }
  # Install and Load Packages
  packages <- c("purrr",
                "rio", 
                "tidyverse",
                "stringr")
  ipak(packages)
```

```{r, load_data, echo = FALSE, warning = FALSE, message = FALSE}
features <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/features.txt") # import features.txt data into working environment
x_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/x_test.txt") # import features.txt data into working environment

```

# Preface 

The Coursera course *Getting and Cleaning Data* CODEBOOK is intended to provide the user with enough information to replicate the data analysis in the project.  Users of the CODEBOOK should be able to understand how the data was tidyed to its current state.  

The `CODEBOOK` is a companion to the `run_analysis.R` and `README.md` files.  The github repository is located at https://github.com/prosumer1001/GettingAndCleaningData_Project.  Direct all questions to the author at prosumer1001@gmail.com.



# Instructions

### Review Criteria

1. The submitted data set is tidy.
2. The Github repo contains the required scripts.
3. GitHub contains a code book that modifies and updates the available codebooks with the data to indicate all the variables and summaries calculated, along with units, and any other relevant information.
4. The README that explains the analysis files is clear and understandable.
5. The work submitted for this project is the work of the student who submitted it.

### Course Project Description 

The course project description is copied verbatim from the site.[^1]

The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit: 1) a tidy data set as described below, 2) a link to a Github repository with your script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called `CodeBook.md`. You should also include a `README.md` in the repo with your scripts. This `README.md` explains how all of the scripts work and how they are connected.

One of the most exciting areas in all of data science right now is wearable computing - see for example <a href="http://www.insideactivitytracking.com/data-science-activity-tracking-and-the-battle-for-the-worlds-top-sports-brand/" target="_blank" rel="noopener nofollow">this article </a>. Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:

http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones

Here are the data for the project:

https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip

You should create one R script called run_analysis.R that does the following.

Merges the training and the test sets to create one data set.
Extracts only the measurements on the mean and standard deviation for each measurement.
Uses descriptive activity names to name the activities in the data set
Appropriately labels the data set with descriptive variable names.
From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

# README

The data codebook can be obtained from the author's .git repository, `GettingAndCleaningData_Project` titled `HARreadme.txt`.  The final tidy dataframe `CODEBOOK.Rmd` that accompany's this project provides detailes for each variable according to the original coding rubric.

# Step 1. Process different datasets.  

First, I take the `raw` datasets and transofrm them into individual `processed` datasets.  The first is the `features.txt` data.  The comments in the code follow the process for creating the first data frame named `complete.RData`, which is saved in the `./Project/Data/Processed` directory.  this step creates the naming convention for the features included in the completed data frame.  The naming conventions form the original features_info.txt file provies the most succinct and descriptive naming.  Quoting from the features_info.txt file:

> The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix 't' to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. 

> Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 

> Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the 'f' to indicate frequency domain signals). 

> These signals were used to estimate variables of the feature vector for each pattern:  
'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.

The difference between the original file and the naming conventions used in this data frame include the removal of all special characters (i.e. **-** and **()**).  Additionally, all characters were changed to lower case.  Thus, a prior exmaple might be:  `fBodyAccJerkMag` and the new variable name is `fbodyaccjerkmag`.  While the lack of upper case letters may make deciphering the variable more difficult at first, it will make errors in the code of future uesers less likely to occur.  The practical benefits is that a major transformation has not taken place, so using the original code-book ensures replicability with less errors, which should increase the likelyhood of replication, all else equal.

The steps used to transform this data are as follows:

1. Load the features and x-test data into the local environment.
2. Remove all punctuation and change characters to lower case.
3. Rename vairalbes and use `select()` to choose only the `featureNames` variable for later use.
4. Make the vector `featureNames` from the prior step into the column names for the `x_test` data frame.
5. Create a new processed data frame called `observationsDF` that will be joined later by the `partialtestDF` data frame.
6. Add an ID row to the data frame for later join with `partialtestDF`.
7. Save the `comleteDF` as an RDataframe for ease of use.



```{r, features, message = FALSE, warning=FALSE}
features <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/features.txt") # import features.txt data into working environment
x_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/x_test.txt") # import features.txt data into working environment
features$V2 <- str_replace_all(features$V2, "[[:punct:]]", "") # remove all punctuation and ensure there are no spaces
features$V2 <- tolower(features$V2)  # make all alpha characters lowercase
features <- features %>%  # change names of variables and select only the strings
        rename(objectid = "V1", featureNames = "V2") %>%
        select(featureNames)
names(x_test) <- features$featureNames # make the vector featureNames the column names for x_test
observationsDF <- x_test # create new processed data frame called `observationsDF` that will ultimately have all the test data included once processed
observationsDF <- tibble::rowid_to_column(observationsDF, "ID")
export(observationsDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/observationsDF.RData") # save processed data to appropriate directory
```


# Step 2. Process the y_test.txt dataset, the activity_labels.txt dataset, and the subject_test.txt data sets.  

These provide a different challenge than the x_test data wrangling in Step 1, which requried transposing a vector and turning it into column names.  In this sequence of data wrangling the subjects, identified by an integer (1-30), needed to be married to the different factor values for walking, walking upstairs, walking downstairs, sitting, standing, and laying down.  The six categories are factors, since one can realistically understanding that lying down is less arduous than ascending stairs.  

The steps needed to complete Step 2 are as follows:

1. Load the subject, y-test data, and activity labels data in the local environment.
2. Create `ID` variables in the y-test and subject data frames so they can be joined by the same.
3. Conduct a `dplyr` `left_join()` by the `ID` variables created in the prior step; for the case of this data frame I decided to call the data frame `partialtestDF`, which will ultimately join with the `observationsDF` from Step 1 above.
4. Rename the variables created by the data import and left join.  The new variable names are `factor` and `subject` in the partialtestDF data frame.  The activity lables data frame will also receive a variable named `factor` that is synonomous with the `partialtestDF` factor variable.  The two data frames will be left joined in the next step by the `factor` variable.  Finally, the activity label's second variable is renamed to `activitycategory`.  The names were chosen to be as simple as possible and follow the naming convention for the x-test variable from Step 1.
5. Conduct a `left_join()` between `activity_labels` data frame and the `partialtestDF` data frame by the `factor` variable.  A check of the data frame with `sample_n()` form `dplyr` verifies the appropriate factors are matched with the activity categories.
6. Finally, exporting the `partialtestDF` to the `./Project/Data/Processed` directory ensures data replication and makes future joins easier to work with.

```{r, factor_observations, message = FALSE, warning=FALSE}
subject_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/subject_test.txt") ## load the subject_text.txt data file
y_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/y_test.txt") ## load the y_test.txt data file
activity_labels <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/activity_labels.txt") ## load the activity_labels.txt data file
# unique(activity_labels)
y_test <- tibble::rowid_to_column(y_test, "ID") ## create a unique variable named `ID` in the y_test for left_join(by = "ID")
subject_test <- tibble::rowid_to_column(subject_test, "ID") ## create a unique variable named `ID` in the subject_test for left_join(by = "ID")
partialtestDF <- left_join(y_test, subject_test, by = "ID") ## left_join y_test and subject_test by = "ID" to make the partialtestDF data frame
partialtestDF <- partialtestDF %>% rename(factor = V1.x) ## rename the V1.x vairable to `factor` for left_join with activity_labels later
partialtestDF <- partialtestDF %>% rename(subject = V1.y)  ## rename V1.y as the subject number for clarity
activity_labels <- activity_labels %>% rename(factor = V1) ## rename the V1 variable from activity_lables to factor for left_join with partialtestDF
activity_labels <- activity_labels %>% rename(activitycategory = V2) ## rename V2 from activity_labels to acticity_category for clear variable description
# str(list(partialtestDF, activity_labels)) 
partialtestDF <- left_join(partialtestDF, activity_labels, by = "factor"); str(partialtestDF)  ## conduct left_join(by = "factor") for the partialtestDF and activity_lables described above; use str() to review the outcome of the join
sample_n(partialtestDF, 25) ## take a sample of the partialtestDF to make sure that the DF is complete and accurate
export(partialtestDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/partialtestDF.RData") # save processed data to appropriate directory

```

# Step 3. Put the data frames together.

The partialtestDF and completeDFs go together easily with matching observation numbers.  Using `left_join()` by the `ID` variable provides the dataframe with the first four variables as highly descriptive being the consecutive `objectid` variable that provides a sequential count of the observations.  It can easily be dropped if not needed, but provides a ready joining variable for future joins.  The `activityfactor` and `activitycategory` variables are matching and become factor variables.  The final descriptive variable that allows data anlysts to tie different factor values of activity and specific subjects is `subjectnumber`.  The `subjectnumber` variable is a numeric code fo the specific test subject.

Variables 5:565 are the collected data.  Analysts can run analysis using the decoding mechanism quoted in Step 1, above.  The steps needed to complete this final step of the data tidying process are outlined following:

1. Import the complete and partial dataframes from Steps 1 and 2.
2. Conduct a `left_join()` by the `ID` variable.
3. Rename the variables so they match the naming convention for the remainder of the dataframe (all lower case descriptive words)
4. Convert appropriate variables to factors instead of characters or numerics.
5. Export the `finaltestDF` to the `./Project/Data/Processed` directory ensures data replication.


```{r, final_dataFrame, message = FALSE, warning=FALSE}
partialtestDF <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/partialtestDF.RData") # import partialtestDF.RData
observationsDF <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/observationsDF.RData") ## import observationsDF.RData
finaltestDF <- left_join(partialtestDF, observationsDF, by = "ID") ## left join by the ID variable
finaltestDF <- finaltestDF %>% rename(activityfactor = factor) ## rename the factor variable to be congruent with dataframe naming convention
finaltestDF <- finaltestDF %>% rename(subjectnumber = subject) ## rename the subject variable to be congruent with dataframe naming convention
finaltestDF <- finaltestDF %>% rename(objectid = ID) ## rename the ID variable to be the objectid variable
str(finaltestDF)
finaltestDF$activityfactor <- as.factor(finaltestDF$activityfactor) ## convert to a factor variable
finaltestDF$activitycategory <- as.factor(finaltestDF$activitycategory) ## convert to a factor variable
finaltestDF <- finaltestDF %>% select(c(objectid, activityfactor, activitycategory, subjectnumber), everything())
export(finaltestDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/finaltestDF.RData") # save processed data to appropriate directory
```



[^1]: from https://www.coursera.org/learn/data-cleaning/peer/FIZtT/getting-and-cleaning-data-course-project

