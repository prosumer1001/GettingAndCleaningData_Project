---
title: "Coursera *Getting and Cleaning Data* Course Project CODEBOOK"
author:  Nick T.
output: html_notebook
---

```{r, load_libraries, echo = FALSE, warning = FALSE, message = FALSE}
  ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
      install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
  }
  # Install and Load Packages
  packages <- c("purrr",
                "rio", 
                "tidyverse",
                "stringr")
  ipak(packages)
```

```{r, load_data, echo = FALSE, warning = FALSE, message = FALSE}
features <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/features.txt") # import features.txt data into working environment
x_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/x_test.txt") # import features.txt data into working environment

```

# Preface 

The Coursera course *Getting and Cleaning Data* CODEBOOK is intended to provide the user with enough information to replicate the data analysis in the project.  Users of the CODEBOOK should be able to understand how the data was tidyed to its current state.  

The `CODEBOOK` is a companion to the `run_analysis.R` and `README.md` files.  The github repository is located at https://github.com/prosumer1001/GettingAndCleaningData_Project.  Direct all questions to the author at prosumer1001@gmail.com.



# Instructions

### Review Criteria

1. The submitted data set is tidy.
2. The Github repo contains the required scripts.
3. GitHub contains a code book that modifies and updates the available codebooks with the data to indicate all the variables and summaries calculated, along with units, and any other relevant information.
4. The README that explains the analysis files is clear and understandable.
5. The work submitted for this project is the work of the student who submitted it.

### Course Project Description 

The course project description is copied verbatim from the site.[^1]

The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit: 1) a tidy data set as described below, 2) a link to a Github repository with your script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called `CodeBook.md`. You should also include a `README.md` in the repo with your scripts. This `README.md` explains how all of the scripts work and how they are connected.

One of the most exciting areas in all of data science right now is wearable computing - see for example <a href="http://www.insideactivitytracking.com/data-science-activity-tracking-and-the-battle-for-the-worlds-top-sports-brand/" target="_blank" rel="noopener nofollow">this article </a>. Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:

http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones

Here are the data for the project:

https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip

You should create one R script called run_analysis.R that does the following.

1. Merges the training and the test sets to create one data set.
2. Extracts only the measurements on the mean and standard deviation for each measurement.
3. Uses descriptive activity names to name the activities in the data set
4. Appropriately labels the data set with descriptive variable names.
5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

# README

The data codebook can be obtained from the author's .git repository, `GettingAndCleaningData_Project` titled `HARreadme.txt`.  The final tidy dataframe `CODEBOOK.Rmd` that accompany's this project provides detailes for each variable according to the original coding rubric.

# Step 1. Import data.  

```{r, data, message = FALSE, warning=FALSE}
## 1. Import Data
        ## 0. Paths
testpath <- "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test"
trainingpath <- "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/training"
testinertials<- "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/InertialSignals"
traininginertials<- "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/training/InertialSignals"
processeddata <- "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/processed"
        ## A. y_test (TEST LABLES)
y_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/y_test.txt") ## load the y_test.txt data file
y_test <- tibble::rowid_to_column(y_test, "ID") ## create a unique variable named `ID` in the y_test for left_join(by = "ID")
        ## B. y_train (TRAINING LABELS)
y_train <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/train/y_train.txt")
y_train <- tibble::rowid_to_column(y_train, "ID") ## create a unique variable named `ID` in the y_train for left_join(by = "ID")
str(y_train)
x_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/x_test.txt") # import features.txt data into working environment
x_test <- tibble::rowid_to_column(x_test, "ID"); names(x_test)
ID <- x_test %>% select(ID); names(ID)
names(x_test)
x_test <- x_test %>% select(-ID); names(x_test)
x_test <- cbind(x_test, ID); names(x_test)

x_train <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/train/x_train.txt") # import features.txt data into working environment
x_train <- tibble::rowid_to_column(x_train, "ID"); names(x_train)
ID <- x_train %>% select(ID); names(ID)
names(x_train)
x_train <- x_train %>% select(-ID); names(x_train)
x_train <- cbind(x_train, ID); names(x_train)

        ## C. subject_test (TEST)
subject_test <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/test/subject_test.txt") ## load the subject_test.txt (the person's ID number that was in the test) data file
subject_test <- tibble::rowid_to_column(subject_test, "ID") ## create a unique variable named `ID` in the subject_test for left_join(by = "ID")
        ## D. subject_train
subject_train <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/train/subject_train.txt") ## load the subject_train.txt data file)
subject_train <- tibble::rowid_to_column(subject_train, "ID") ## create a unique variable named `ID` in the subject_train for left_join(by = "ID")

        ## E. activity_lables
activity_labels <- import("~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Raw/activity_labels.txt") ## load the activity_labels.txt data file
unique(activity_labels)
ID <- "ID"
features <- rbind(features, ID); tail(features)
```

# Step 2. Process data frames into untidy data frame.

## Variable name rules / coding rules.  

The `raw` datasets exist in eight text files:

* x_train
* y_train
* subject_train
* x_test
* y_test
* subject_test
* features
* activity_labels

The x_train and x_test files containe all of the observational data.  The y_train and y_test files contain the subject number by observation row.  The features text file provides variable names for the 561 unique variables for the experimentation in training and testing.  The activity labels text file provides the factor levels for the different observations.  For example, observation 558 is a walking variable, which needs to be accurately matches with the x_train and x_test observations.

> The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix 't' to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. 

> Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 

> Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the 'f' to indicate frequency domain signals). 

> These signals were used to estimate variables of the feature vector for each pattern:  
'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.

The difference between the original file and the naming conventions used in this data frame include the removal of all special characters (i.e. **-** and **()**).  Additionally, all characters were changed to lower case.  Thus, a prior exmaple might be:  `fBodyAccJerkMag` and the new variable name is `fbodyaccjerkmag`.  While the lack of upper case letters may make deciphering the variable more difficult at first, it will make errors in the code of future uesers less likely to occur.  The practical benefits is that a major transformation has not taken place, so using the original code-book ensures replicability with less errors, which should increase the likelyhood of replication, all else equal.


```{r, untidy_data, echo = TRUE, message = FALSE, warning=FALSE}
## 2. Create Activity and Subject Data Frames (activityDF v1.0 and subjectDF v1.0)
##      A. Create Activity Labels Data Frame for Joining to y_test and y_train
y_test <- y_test %>% rename(label = V1) ## rename variable for future join
y_train <- y_train %>% rename(label = V1) ## rename variable for future join
activity_labels <- activity_labels %>% rename(label = V1) ## rename variable for future join
activity_labels <- activity_labels %>% rename(category = V2) ## rename variable for future join

##      B. LEFT JOIN 01 - y_test + activity_labels to make activityDF01 - goes with test data
activityDF01 <- left_join(y_test, activity_labels, by = "label"); str(activityDF01)  ## conduct left_join(by = "factor") for the partialDF and activity_lables described above; use str() to review the outcome of the join
sample_n(activityDF01, 10) ## take a sample of the partialDF to make sure that the DF is complete and accurate
activityDF01 <- activityDF01 %>% rename(testlabel = label) ## rename variable for future join
activityDF01 <- activityDF01 %>% rename(testcategory = category) ## rename variable for future join

##      C. LEFT JOIN 02 - y_train + activity_labels to make activityDF02 - goes with training data
activityDF02 <- left_join(y_train, activity_labels, by = "label"); str(activityDF02)  ## conduct left_join(by = "factor") for the partialDF and activity_lables described above; use str() to review the outcome of the join
sample_n(activityDF02, 10) ## take a sample of the partialDF to make sure that the DF is complete and accurate
activityDF02 <- activityDF02 %>% rename(traininglabel = label)
activityDF02 <- activityDF02 %>% rename(trainingcategory = category)

##      D. FULL JOIN 01 - activityDF01 + activityDF02 = activityDF (version 1.0)
activityDF <- full_join(activityDF01, activityDF02, by = "ID")
# sample_n(activityDF, 25) ## take a sample of the partialDF to make sure that the DF is complete and accurate
# tail(activityDF)

##      E. FULL JOIN 02 - subject_test + subject_train = subjectDF (version 1.0)
subject_test <- subject_test %>% rename(testsubjectid = V1)
subject_train <- subject_train %>% rename(trainingsubjectid = V1)
subjectDF <- full_join(subject_test, subject_train, by = "ID") ## left_join subject_test and subject_train by = "ID" to make subjectDF (version 1.0)
# sample_n(subjectDF, 25) ## take a sample of the partialDF to make sure that the DF is complete and accurate
# tail(subjectDF)

##      F. Create a vector from the features.txt data set that will become the variable
##         names for the x_train and x_test in the next step.
features$V2 <- str_replace_all(features$V2, "[[:punct:]]", "") # remove all punctuation and ensure there are no spaces
features$V2 <- tolower(features$V2)  # make all alpha characters lowercase
features <- features %>%  # change names of variables and select only the strings
        rename(objectid = "V1", featureNames = "V2") %>%
        select(featureNames); sample_n(features, 10); tail(features)

##      G. Make the features.txt names into the variable names for the x_train and x_test data frames.
names(x_test) <- features$featureNames; head(x_test); names(x_test) # make the vector featureNames the column names for x_test
names(x_train) <- features$featureNames; head(x_train); names(x_train) # make the vector featureNames the column names for x_train
tail(x_test)
tail(x_train)

```

# Step 3. Process the y_test.txt dataset, the activity_labels.txt dataset, and the subject_test.txt data sets.  

There will be many variables since both the training and the test data have the same variables--each of which has 561 variables. The approach used is to create a tidy test variable and training variable.  After the training data frame is made tidy it can be bound to the tidy test data frame by rows.

```{r, process_data, echo = TRUE, message=FALSE, warning=FALSE}
completetestDF <- x_test # create new processed data frame called `completetestDF` that will ultimately have all the test data included once processed
export(completetestDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/completetestDF.RData") # save processed data to appropriate directory

completetrainDF <- x_train # create new processed data frame called `completeDF` that will ultimately have all the test data included once processed
completetrainDF
export(completetrainDF, file = "~/Documents/gitrepos/DataAnalysis/GettingAndCleaningData_Project/Project/Data/Processed/completetrainDF.RData")


```





[^1]: from https://www.coursera.org/learn/data-cleaning/peer/FIZtT/getting-and-cleaning-data-course-project

